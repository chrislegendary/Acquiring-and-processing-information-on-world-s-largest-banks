{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e8f10c-9ebe-4c90-a3d4-c31b5d0db0cd",
   "metadata": {},
   "source": [
    "# Acquiring and Processing Information on the World's Largest Banks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf1811f-93d0-42e9-aca1-75366e786e14",
   "metadata": {},
   "source": [
    "In this project, we will work with real-world data and perform the operations of Extraction, Transformation, and Loading (ETL) as required.\n",
    "\n",
    "* Task 1: Logging function\n",
    "* Task 2: Extraction of data\n",
    "* Task 3: Transformation of data\n",
    "* Task 4: Loading to CSV\n",
    "* Task 5: Loading to Database\n",
    "* Task 6: Function to Run queries on Database\n",
    "* Task 7: Verify log entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bf7ea-0ea4-4e63-818e-70c22a8e1529",
   "metadata": {},
   "source": [
    "### Preliminaries: Installing libraries and downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff370e8-f0cb-4d6c-b25a-bca4ccaee79c",
   "metadata": {},
   "source": [
    "Install the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2144e30c-74b9-4433-822b-4c5831481070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\chris\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\chris\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting bs4\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting wget\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from bs4) (4.12.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\chris\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: wget, bs4\n",
      "Successfully installed bs4-0.0.2 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas bs4 wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998b744f-b237-4f6f-9127-76fef1519645",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c52650b0-e6b6-4362-bcc9-f7c465e78041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# Suppress generated warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef00654-0acc-4c4f-827f-a59ce16a4073",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "Task 1: Logging function\n",
    "This function accepts the message to be logged and enters it to the log file on a new line.\n",
    "\n",
    "Log entries will be in the format: <timestamp> : <message>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30b6b701-1dfc-4dae-9f4b-860329d6e76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_progress(message):\n",
    "    ''' This function logs the mentioned message of a given stage of the\n",
    "    code execution to a log file. Function returns nothing'''\n",
    "\n",
    "    timestamp_format = \"%Y-%h-%d-%H:%M:%S\" # Year-Monthname-Day-Hour-Minute-Second\n",
    "    now = datetime.now() # get current timestamp\n",
    "    timestamp = now.strftime(timestamp_format)\n",
    "    with open(log_file,\"a\") as f:\n",
    "        f.write(timestamp + \" : \" + message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07cf24-445d-4022-b08b-62ae1513383d",
   "metadata": {},
   "source": [
    "Task 2: Extraction of data\n",
    "\n",
    "Inspect the URL and identify the position and pattern of the tabular information in the HTML code under the heading By market capitalization. In the given webpage, our table is at the first position, or index 0 and we require the entries under \"Bank name\" and \"Market cap (US$ billion)\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9c1a1-546d-4c13-806e-cf284d674825",
   "metadata": {},
   "source": [
    "Write the function extract() to retrieve the information of the table to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15bdd0c9-be62-4f9e-ad95-18c5e65648ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(url, table_attribs):\n",
    "    ''' This function aims to extract the required\n",
    "    information from the website and save it to a data frame. The\n",
    "    function returns the data frame for further processing. '''\n",
    "\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    df = pd.DataFrame(columns=table_attribs)\n",
    "\n",
    "    tables = soup.find_all(\"tbody\")\n",
    "    rows = tables[0].find_all(\"tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        col = row.find_all(\"td\")\n",
    "        if len(col) != 0:\n",
    "            data_dict = {\"Name\": col[1].find_all(\"a\")[1][\"title\"],\n",
    "                         \"MC_USD_Billion\": float(col[2].contents[0][:-1])}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1be1c2-3550-4139-9ce0-072add7b8c77",
   "metadata": {},
   "source": [
    "The code functions as follows:\n",
    "\n",
    "Extract the web page as text.\n",
    "Parse the text into an HTML object.\n",
    "Create an empty Pandas DataFrame named df with columns argument set as table_attribs.\n",
    "Extract all tbody attributes of the HTML object and then extract all the rows of the index 0 table using the tr attribute.\n",
    "Iterate over the contents of the variable rows.\n",
    "Extract all the td data objects in the row and save them to col.\n",
    "Check if the length of col is 0, that is, if there is no data in the current row. This is important since, many times there are merged rows that are not apparent in the web page appearance.\n",
    "Create a dictionary data_dict with the keys same as the columns of the dataframe created for recording the output earlier and corresponding values from the second and third headers of data.\n",
    "Extract the title attribute of the second hyperlink from the Bank name column.\n",
    "Remove the last character (\\n) from the Market cap (US$ billion) column contents using negative index slicing, then typecast the value to float format.\n",
    "Convert the dictionary to a dataframe and concatenate it with the existing one. This way, the data keeps getting appended to the dataframe with every iteration of the loop.\n",
    "Return the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23c215-aab5-417e-a957-11f4708b4940",
   "metadata": {},
   "source": [
    "## Task 3: Transformation of data\n",
    "The Transform function needs to perform the following tasks:\n",
    "\n",
    "Read the exchange rate CSV file and convert the contents to a dictionary so that the contents of the first columns are the keys to the dictionary and the contents of the second column are the corresponding values.\n",
    "\n",
    "Add 3 different columns to the dataframe: MC_GBP_Billion, MC_EUR_Billion and MC_INR_Billion, each containing the content of MC_USD_Billion scaled by the corresponding exchange rate factor. Round the resulting data to 2 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5a6b52-42a7-499e-9089-53a625e6f5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df, csv_path):\n",
    "    ''' This function accesses the CSV file for exchange rate\n",
    "    information, and adds three columns to the data frame, each\n",
    "    containing the transformed version of Market Cap column to\n",
    "    respective currencies'''\n",
    "\n",
    "    # Read exchange rate CSV file\n",
    "    exchange_rate = pd.read_csv(csv_path)\n",
    "\n",
    "    # Convert to a dictionary with \"Currency\" as keys and \"Rate\" as values\n",
    "    exchange_rate = exchange_rate.set_index(\"Currency\").to_dict()[\"Rate\"]\n",
    "\n",
    "    # Add MC_GBP_Billion, MC_EUR_Billion, and MC_INR_Billion\n",
    "    # columns to dataframe. Round off to two decimals\n",
    "    df[\"MC_GBP_Billion\"] = [np.round(x * exchange_rate[\"GBP\"], 2) for x in df[\"MC_USD_Billion\"]]\n",
    "    df[\"MC_EUR_Billion\"] = [np.round(x * exchange_rate[\"EUR\"], 2) for x in df[\"MC_USD_Billion\"]]\n",
    "    df[\"MC_INR_Billion\"] = [np.round(x * exchange_rate[\"INR\"], 2) for x in df[\"MC_USD_Billion\"]]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7426960-bc2e-4496-bd70-9d55c061ca91",
   "metadata": {},
   "source": [
    "### Task 4: Loading to CSV\n",
    "Load the transformed dataframe to an output CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025a73eb-64b0-4307-85dc-f8f824a7fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_csv(df, output_path):\n",
    "    ''' This function saves the final data frame as a CSV file in\n",
    "    the provided path. Function returns nothing.'''\n",
    "\n",
    "    df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a955ce3-0018-491b-9e14-6afd80439c5e",
   "metadata": {},
   "source": [
    "### Task 5: Loading to Database\n",
    "Load the transformed dataframe to a SQL database server as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a18ba4-fa26-42c2-ac23-37eb507db797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_to_db(df, sql_connection, table_name):\n",
    "    ''' This function saves the final data frame to a database\n",
    "    table with the provided name. Function returns nothing.'''\n",
    "\n",
    "    df.to_sql(table_name, sql_connection, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b265c139-a978-4073-8612-7cfb159cd69b",
   "metadata": {},
   "source": [
    "## Task 6: Function to Run queries on Database\n",
    "Run queries on the database table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccfdbaf0-61a5-4710-b296-bc3027fe3d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query_statement, sql_connection):\n",
    "    ''' This function runs the query on the database table and\n",
    "    prints the output on the terminal. Function returns nothing. '''\n",
    "\n",
    "    print(query_statement)\n",
    "    query_output = pd.read_sql(query_statement, sql_connection)\n",
    "    print(query_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f68ad0-7db0-46af-a1f2-516f30cd2700",
   "metadata": {},
   "source": [
    "## Running ETL Process\n",
    "Declaring known values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac863a7a-43dc-4640-b019-1e26aa0d8dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preliminaries complete. Initiating ETL process\n"
     ]
    }
   ],
   "source": [
    "url = \"https://web.archive.org/web/20230908091635/https://en.wikipedia.org/wiki/List_of_largest_banks\"\n",
    "csv_path = \"./exchange_rate.csv\"\n",
    "table_attribs = [\"Name\", \"MC_USD_Billion\"]\n",
    "output_path = \"./Largest_banks_data.csv\"\n",
    "db_name = \"Banks.db\"\n",
    "table_name = \"Largest_banks\"\n",
    "log_file = \"./code_log.txt\"\n",
    "\n",
    "import logging\n",
    "\n",
    "# Define the log_progress function\n",
    "def log_progress(message):\n",
    "    logging.basicConfig(filename=log_file, level=logging.INFO)\n",
    "    logging.info(message)\n",
    "    print(message)  # Optional: print the message to the console for visibility\n",
    "\n",
    "# Call the function\n",
    "log_progress(\"Preliminaries complete. Initiating ETL process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5382e729-9e57-4d6c-b8bb-5607038aecf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion\n",
      "0                           JPMorgan Chase          432.92\n",
      "1                          Bank of America          231.52\n",
      "2  Industrial and Commercial Bank of China          194.56\n",
      "3               Agricultural Bank of China          160.68\n",
      "4                                HDFC Bank          157.91\n",
      "5                              Wells Fargo          155.87\n",
      "6                                     HSBC          148.90\n",
      "7                           Morgan Stanley          140.83\n",
      "8                  China Construction Bank          139.82\n",
      "9                            Bank of China          136.81\n",
      "Data extraction complete. Initiating Transformation process\n"
     ]
    }
   ],
   "source": [
    "df = extract(url, table_attribs)\n",
    "print(df)\n",
    "\n",
    "log_progress(\"Data extraction complete. Initiating Transformation process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c15bc3f9-f09f-4018-899e-806b19e6fb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Name  MC_USD_Billion  MC_GBP_Billion  \\\n",
      "0                           JPMorgan Chase          432.92          346.34   \n",
      "1                          Bank of America          231.52          185.22   \n",
      "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
      "3               Agricultural Bank of China          160.68          128.54   \n",
      "4                                HDFC Bank          157.91          126.33   \n",
      "5                              Wells Fargo          155.87          124.70   \n",
      "6                                     HSBC          148.90          119.12   \n",
      "7                           Morgan Stanley          140.83          112.66   \n",
      "8                  China Construction Bank          139.82          111.86   \n",
      "9                            Bank of China          136.81          109.45   \n",
      "\n",
      "   MC_EUR_Billion  MC_INR_Billion  \n",
      "0          402.62        35910.71  \n",
      "1          215.31        19204.58  \n",
      "2          180.94        16138.75  \n",
      "3          149.43        13328.41  \n",
      "4          146.86        13098.63  \n",
      "5          144.96        12929.42  \n",
      "6          138.48        12351.26  \n",
      "7          130.97        11681.85  \n",
      "8          130.03        11598.07  \n",
      "9          127.23        11348.39  \n",
      "Data transformation complete. Initiating Loading process\n"
     ]
    }
   ],
   "source": [
    "df = transform(df, csv_path)\n",
    "print(df)\n",
    "\n",
    "log_progress(\"Data transformation complete. Initiating Loading process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2bc9660-897e-4bdc-a617-3e8d71ab667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to CSV file\n",
      "Data loaded into SQLite3 database\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (df) and output path\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Bank A\", \"Bank B\", \"Bank C\"],\n",
    "    \"MC_USD_Billion\": [500, 450, 400]\n",
    "})\n",
    "output_path = \"./Largest_banks_data.csv\"\n",
    "db_name = \"Banks.db\"\n",
    "table_name = \"Largest_banks\"\n",
    "\n",
    "# Function to save the DataFrame to CSV\n",
    "def load_to_csv(df, output_path):\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "load_to_csv(df, output_path)\n",
    "log_progress(\"Data saved to CSV file\")\n",
    "\n",
    "# Initiate SQLite3 connection\n",
    "conn = sqlite3.connect(db_name)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table in SQLite if it doesn't exist\n",
    "cursor.execute(f'''\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        Name TEXT,\n",
    "        MC_USD_Billion REAL\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Insert data into the table\n",
    "for index, row in df.iterrows():\n",
    "    cursor.execute(f'''\n",
    "        INSERT INTO {table_name} (Name, MC_USD_Billion)\n",
    "        VALUES (?, ?)\n",
    "    ''', (row['Name'], row['MC_USD_Billion']))\n",
    "\n",
    "# Commit the transaction and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "log_progress(\"Data loaded into SQLite3 database\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf36ea8-2112-4fcc-a99e-6b65edc678f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Connection initiated\n",
      "Data loaded into SQLite database\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame to be inserted into the database\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Bank A\", \"Bank B\", \"Bank C\"],\n",
    "    \"MC_USD_Billion\": [500, 450, 400]\n",
    "})\n",
    "\n",
    "# Path to the SQLite database\n",
    "db_name = \"Banks.db\"\n",
    "table_name = \"Largest_banks\"\n",
    "\n",
    "# Function to load data into SQLite database\n",
    "def load_to_db(df, db_name, table_name):\n",
    "    '''This function loads data from the DataFrame into the SQLite database.'''\n",
    "    \n",
    "    # Establish SQLite connection\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it doesn't exist\n",
    "    cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            Name TEXT,\n",
    "            MC_USD_Billion REAL\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Insert data into the table\n",
    "    for index, row in df.iterrows():\n",
    "        cursor.execute(f'''\n",
    "            INSERT INTO {table_name} (Name, MC_USD_Billion)\n",
    "            VALUES (?, ?)\n",
    "        ''', (row['Name'], row['MC_USD_Billion']))\n",
    "    \n",
    "    # Commit and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Establish SQL connection\n",
    "sql_connection = sqlite3.connect(db_name)\n",
    "log_progress(\"SQL Connection initiated\")  # Log the progress\n",
    "\n",
    "# Call the function to load data into the database\n",
    "load_to_db(df, db_name, table_name)  # No 'Call' keyword needed\n",
    "\n",
    "log_progress(\"Data loaded into SQLite database\")  # Log after loading the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "822932b4-7abc-43a0-af9d-4a758af6dfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL Connection initiated\n",
      "Data loaded to Database as a table, Executing queries\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame (df)\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Bank A\", \"Bank B\", \"Bank C\"],\n",
    "    \"MC_USD_Billion\": [500, 450, 400]\n",
    "})\n",
    "\n",
    "# Path to the SQLite database\n",
    "db_name = \"Banks.db\"\n",
    "table_name = \"Largest_banks\"\n",
    "\n",
    "# Function to load data into SQLite database (using the active connection)\n",
    "def load_to_db(df, conn, table_name):\n",
    "    '''This function loads data from the DataFrame into the SQLite database using an existing connection.'''\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table if it doesn't exist\n",
    "    cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "            Name TEXT,\n",
    "            MC_USD_Billion REAL\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Insert data into the table\n",
    "    for index, row in df.iterrows():\n",
    "        cursor.execute(f'''\n",
    "            INSERT INTO {table_name} (Name, MC_USD_Billion)\n",
    "            VALUES (?, ?)\n",
    "        ''', (row['Name'], row['MC_USD_Billion']))\n",
    "    \n",
    "    # Commit the transaction\n",
    "    conn.commit()\n",
    "\n",
    "# Establish SQL connection\n",
    "sql_connection = sqlite3.connect(db_name)\n",
    "log_progress(\"SQL Connection initiated\")  # Log the progress\n",
    "\n",
    "# Call the function to load data into the database (use the existing connection)\n",
    "load_to_db(df, sql_connection, table_name)\n",
    "\n",
    "log_progress(\"Data loaded to Database as a table, Executing queries\")  # Log after loading the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447d780-6ff0-42c2-ae31-4d0e5543c01f",
   "metadata": {},
   "source": [
    "##1. Print the contents of the entire table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19d5745d-22c3-4ac5-bb47-00eb74414b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract(url, table_attribs):\n",
    "    # Send request to the URL and parse the content\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find the table based on the given attributes\n",
    "    table = soup.find('table', attrs=table_attribs)\n",
    "    \n",
    "    # Parse the table into a DataFrame\n",
    "    df = pd.read_html(str(table))[0]  # Assuming there's only one table on the page\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "03137b09-96e8-4260-b2ce-d98a4386448e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2086909656.py, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 32\u001b[1;36m\u001b[0m\n\u001b[1;33m    Name  MC_USD_Billion  MC_GBP_Billion\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming extract and log_progress are defined elsewhere\n",
    "\n",
    "# Extract data using the extract() function\n",
    "df = extract(url, table_attribs)\n",
    "\n",
    "# Print the extracted data to verify\n",
    "print(df)\n",
    "\n",
    "# Log progress\n",
    "log_progress(\"Data extraction complete. Initiating Transformation process.\")\n",
    "\n",
    "# Define the transform() function to process the data\n",
    "def transform(df):\n",
    "    # Example transformation: Convert USD market cap to millions (instead of billions)\n",
    "    df['MC_USD_Million'] = df['MC_USD_Billion'] * 1000  # Convert billions to millions\n",
    "    \n",
    "    # Example transformation: Add a column that indicates if market cap is over 200 billion\n",
    "    df['High_Market_Cap'] = df['MC_USD_Billion'] > 200\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Call the transform function to process the extracted data\n",
    "transformed_df = transform(df)\n",
    "\n",
    "# Print the transformed data to verify\n",
    "print(transformed_df)\n",
    "\n",
    "\n",
    "\n",
    "                                      Name  MC_USD_Billion  MC_GBP_Billion  \n",
    "0                           JPMorgan Chase          432.92          346.34   \n",
    "1                          Bank of America          231.52          185.22   \n",
    "2  Industrial and Commercial Bank of China          194.56          155.65   \n",
    "3               Agricultural Bank of China          160.68          128.54   \n",
    "4                                HDFC Bank          157.91          126.33   \n",
    "5                              Wells Fargo          155.87          124.70   \n",
    "6                                     HSBC          148.90          119.12   \n",
    "7                           Morgan Stanley          140.83          112.66   \n",
    "8                  China Construction Bank          139.82          111.86   \n",
    "9                            Bank of China          136.81          109.45   \n",
    "\n",
    "   MC_EUR_Billion       MC_INR_Billion  \n",
    "0          402.62        35910.71  \n",
    "1          215.31        19204.58  \n",
    "2          180.94        16138.75  \n",
    "3          149.43        13328.41  \n",
    "4          146.86        13098.63  \n",
    "5          144.96        12929.42  \n",
    "6          138.48        12351.26  \n",
    "7          130.97        11681.85  \n",
    "8          130.03        11598.07  \n",
    "9          127.23        11348.39  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f3123d-45e0-4652-aef7-ebad432586d6",
   "metadata": {},
   "source": [
    "2. Print the average market capitalization of all the banks in Billion GBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "054df62a-17e7-4597-8a8b-bff61004a3c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (556639637.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[44], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    SELECT AVG(MC_GBP_Billion) FROM Largest_banks;\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "SELECT AVG(MC_GBP_Billion) FROM Largest_banks;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cbc262-abc9-4cdd-a115-ba3e758ef038",
   "metadata": {},
   "source": [
    "3.Print only the names of the top 5 banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9af3a998-f814-45a2-aaaf-3b046a67d715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Name from Largest_banks LIMIT 5\n",
      "     Name\n",
      "0  Bank A\n",
      "1  Bank B\n",
      "2  Bank C\n",
      "3  Bank A\n",
      "4  Bank B\n",
      "Process Complete\n"
     ]
    }
   ],
   "source": [
    "query_statement = f\"SELECT Name from {table_name} LIMIT 5\"\n",
    "run_query(query_statement, sql_connection)\n",
    "\n",
    "log_progress(\"Process Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee7791c-dcda-4b1b-bdab-257cad181296",
   "metadata": {},
   "source": [
    "## Task 7: Verify log entries\n",
    "Upon successful completion of execution, you should see all the relevant entries made in the log file in relation to the stages of code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50e0c777-4e9d-4090-88ed-071e00007141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Preliminaries complete. Initiating ETL process\n",
      "INFO:numexpr.utils:NumExpr defaulting to 4 threads.\n",
      "INFO:root:Data extraction complete. Initiating Transformation process\n",
      "INFO:root:Data transformation complete. Initiating Loading process\n",
      "INFO:root:Data saved to CSV file\n",
      "INFO:root:Data loaded into SQLite3 database\n",
      "INFO:root:SQL Connection initiated\n",
      "INFO:root:Data loaded into SQLite database\n",
      "INFO:root:SQL Connection initiated\n",
      "INFO:root:Data loaded to Database as a table, Executing queries\n",
      "INFO:root:SQL Connection initiated\n",
      "INFO:root:Data loaded to Database as a table, Executing queries\n",
      "INFO:root:Process Complete\n",
      "INFO:root:Preliminaries complete. Initiating ETL process\n",
      "INFO:root:Process Complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(log_file, \"r\") as log:\n",
    "    LogContent = log.read()\n",
    "    print(LogContent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3307ee37-cecd-4415-a5b0-1503e626670c",
   "metadata": {},
   "source": [
    "## Christian Reyes\n",
    "11/16/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd9d3f3-71a8-4a6d-9271-4d5b891ad238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
